% paper 3
\subsection[Presentation of Ntoutsi et al. (2020)]{Presentation of \textcite{Ntoutsi_2020}}
The third paper, \citetitle{Ntoutsi_2020} by \textcite{Ntoutsi_2020}, is a qualitative analysis of existing approaches for handling bias and fairness in artificial intelligence systems. The first section of the paper starts by introducing the concept of bias, including a brief history of bias within the field of artificial intelligence, a definition of bias to be used throughout the paper, and causes of bias. The section goes on to discuss how bias is represented in data, before it discusses the definition of fairness -- and related challenges. Finally, the first section rounds out by discussing legal issues related to bias and fairness in artificial intelligence systems.

The second part of the paper discusses ways to mitigate bias. The mitigation methods are divided in three. Preprocessing approaches look at data, and how bias can be mitigated by adjusting the dataset being supplied into an AI model. In-processing approaches looks at the model itself, and how bias mitigation can be made part of the core of the process. Post-processing models look at how bias can be mitigated by adjusting the outputs of the model, after it has made a decision, but before the decision is finalized. The part is finalized by a discussion of legal issues related to bias mitigation.

The final part of the paper looks at ways to handle bias in an AI system. It starts by looking at ways to proactively handle bias, by being aware of it when collecting data, concluding that it is nearly impossible to guarantee completely bias-free data collection methods. Next, methods for how to describe bias is discussed,  primarily focused on the use of ontologies -- formal descriptions and representations of bias in a system. Finally, methods for retroactively describing decisions made by an AI system are discussed, highlighting many of the same methods discussed by \textcite{Barredo_2020}, before a discussion of legal issues related to handling bias. The paper is rounded out by a discussion of the direction of bias research going forward.



\subsection[Evaluation of Ntoutsi et al. (2020)]{Evaluation of \textcite{Ntoutsi_2020}}
% Does the paper contains explicit research questions? What are they? If not, what is the goal of the research?
Following the recommendations from \textcite{Cuschieri_2019,Davidson_2012}, \textcite{Ntoutsi_2020} include an explicit formulation of the overarching goal of their research. Introduced in the abstract, the goal of \textcite[p.1]{Ntoutsi_2020} is to "provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well-grounded in a legal frame." Including this in the abstract aligns with \textcite[p.115]{Cuschieri_2019}, who state that the abstract should include "the purpose for conducting the research," as well as \textcite[p.359]{Davidson_2012}, who recommend that the abstract should "succinctly describe the aim of the study." On the other hand, \textcite{Jha_2014,Katz_2006,Lin_2012,Rosenfeldt_2000} suggests moving the aim to the introduction, instead prioritizing the results and conclusions of the research in the abstract. \textcite[p.115]{Cuschieri_2019} also recommend to include the results and main conclusion in the abstract, alongside the aim. Neither results nor conclusions are mentioned in the abstract of \textcite{Ntoutsi_2020}.

Although they explicitly state their research goal, \textcite{Ntoutsi_2020} do not mention their research questions anywhere in their paper. This makes the reading process similar to the one mentioned in \autoref{sec:vakkuri-evaluation}, where the author's questions must be understood from the stated research goal and the parts of the text where research questions are expected to be involved -- the title \parencite[p.359]{Davidson_2012}, the abstract \parencite[p.115]{Cuschieri_2019} and the introduction \parencite[p.2]{Jha_2014}. While the introduction of \textcite{Ntoutsi_2020} does not contain explicit research questions, it does contain a division of their work into three categories: \textit{Understanding bias}, \textit{Mitigating bias} and \textit{Accounting for bias} \parencite[p.2]{Ntoutsi_2020}. Much like the process described in \autoref{sec:vakkuri-evaluation}, these three categories can be used as foundation for three potential research questions (PRQs):

\begin{table}[h]
    \centering
    \begin{tabular}{cl}
        PRQ 1 & What is bias, and how does it affect AI systems? \\
        PRQ 2 & How can bias in AI systems be mitigated? \\
        PRQ 3 & How can bias in AI systems be accounted for? \\
    \end{tabular}
    % \caption{Potential research questions for \textcite{Ntoutsi_2020}.}
    % \label{tab:ntoutsi_PRQs}
\end{table}

In order to be able to use these potential research questions throughout the evaluation, they must align with the overarching research goal, as well as the two remaining sections mentioned -- the title and the abstract. The abstract of \textcite{Ntoutsi_2020} has already been discussed, as it contains the overarching goal of the research. As answering the presented PRQs gives a broad overview of the field of bias in AI systems, the goal -- and thus the abstract -- closely aligns with the presented PRQs. Similarly, the title of \textcite{Ntoutsi_2020}, which reads "Bias in data-driven artificial intelligence systems-â€”An introductory survey," is almost a repetition of the research goal, and thus closely aligns with the presented PRQs. We therefore continue the evaluation using PRQ 1-3 as representative research questions for the paper.

% How can this paper's use of RQs contribute to mitigating "publish or perish"?
% They don't have any, so it can't really

% How does the results connect with RQs?
As the PRQs are inferred from the categories put forward by \textcite{Ntoutsi_2020}, the connections between them and the results found in the paper are clear, with each PRQ being answered by a specific section. Section 2, discussing bias, aims to answer PRQ 1. The section proposes a definition for bias, thus answering the question of "What is bias," before answering the rest of the PRQ by looking at ways bias impacts AI systems. Section 3 discusses ways to mitigate bias, and thus fully answer PRQ2. Likewise, section 4 discusses ways to account for bias in AI systems, thus fully answering PRQ 3. Although no explicit mentions of the categories are made, the connection is still implicitly there, as the results align perfectly with the PRQs. The paper does not mention the PRQs in the results, which is natural, as they are not introduced in the paper, but instead developed by the reader.

% How does the conclusion connect with RQs?
The conclusion of \textcite{Ntoutsi_2020} is primarily focused on the path for future research. As such, it naturally focuses on how current approaches to bias mitigation and handling can be reviewed and improved, thus aligning itself with PRQ 2 and 3. The conclusion calls for "a systematic evaluation of the existing approaches" for mitigating bias and the creation of "benchmark datasets" for testing bias mitigation \parencite[p.9-10]{Ntoutsi_2020}, both of which would further contribute to answering PRQ~2. Similarly, recommendations are made for researching bias handling and mitigation in more complicated AI systems, such as unsupervised or reinforcement learning, which would contribute to answering both PRQ 2 and 3. Finally, \textcite[p.10]{Ntoutsi_2020} argues that people working on AI systems should "be aware of bias-related issues and the effect of their design choices and assumptions," and that "it is a key responsibility of technology creators to understand [technology's] limits and to propose safeguards to avoid pitfalls." These can both be considered preemptive bias mitigation or bias accounting, thus directly connecting with PRQ 2 and 3.

The connection between the PRQ 1 and the conclusion, however, is not as clear. Three such connections were found throughout the concluding section. First, the conclusion discusses differing definitions of fairness, and how collectively agreeing on a given definition is difficult. As bias is defined as "inclination or prejudice of a decision [...] which is for or against one person or group, especially in a way considered to be unfair" \textcite[p.3]{Ntoutsi_2020}, having a clear definition of fairness -- and thus unfairness -- affects the definition of bias, connecting it to the first part of PRQ 1. Second, \textcite[p.10]{Ntoutsi_2020} stress that "everyone involved in the [AI] decision making process should be aware of bias-related issues." While this does not answer PRQ 1, it connects back to the second part of the question, i.e., how bias affects AI systems. Finally, \textcite[p.10]{Ntoutsi_2020} state that "biases are deeply embedded in our societies," and that the problem of bias cannot be solved by technology alone. By discussing where biases come from, as well as how they are found throughout society, this connects back to both parts of PRQ 1.


% Conclusion
In conclusion, \textcite{Ntoutsi_2020} explicitly states the overarching goal of their research in the abstract, thus making it easy for readers to understand what the authors are trying to achieve. Although the authors do not explicitly mention their research questions, they are relatively simple to discern from the stated overarching goal, combined with the categorization presented in the introduction. Doing so presents three potential research questions, here named PRQ 1-3. The results of the research, as well as the conclusions drawn by \textcite{Ntoutsi_2020}, closely aligns with the research questions. This signals that the PRQs are close to the original goals of the authors, and shows a clear alignment between the stated overarching research goal and the work done in the paper.